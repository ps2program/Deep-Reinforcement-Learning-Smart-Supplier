 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `---------------Mandatory Information to fill------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group ID:\n",
    "### Group Members Name with Student ID:\n",
    "1. Student 1\n",
    "2. Student 2\n",
    "3. Student 3\n",
    "4. Student 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-------------------Write your remarks (if any) that you want should get consider at the time of evaluation---------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks: This implementation includes comprehensive documentation, type hints, and efficient numpy operations for better performance and readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize constants and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "N_ITERATIONS = 1000\n",
    "EPSILON_VALUES = [0.1, 0.2, 0.5]\n",
    "UCB_C = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset (0.5M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and display dataset statistics\n",
    "data = pd.read_csv('Clinical_Trial.csv')\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(data.describe())\n",
    "print(\"\\nTreatment Distribution:\")\n",
    "print(data['trt'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design a Clinical Trial Environment (0.5M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ClinicalTrialEnvironment:\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.n_arms = 4  # Number of treatment arms\n",
    "        self.current_index = 0\n",
    "        self.total_patients = len(self.data)\n",
    "        \n",
    "    def get_reward(self, arm: int) -> float:\n",
    "        \"\"\"Calculate reward based on CD4 count improvement and survival\"\"\"\n",
    "        if self.current_index >= self.total_patients:\n",
    "            self.current_index = 0\n",
    "            \n",
    "        patient = self.data.iloc[self.current_index]\n",
    "        self.current_index += 1\n",
    "        \n",
    "        # Check if treatment matches and calculate reward\n",
    "        if patient['trt'] == arm:\n",
    "            if patient['label'] == 0 and patient['cd420'] > patient['cd40']:\n",
    "                return 1.0\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Random Policy (0.5M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class RandomPolicy:\n",
    "    def __init__(self, n_arms: int):\n",
    "        self.n_arms = n_arms\n",
    "        self.arm_counts = np.zeros(n_arms)\n",
    "        self.arm_rewards = np.zeros(n_arms)\n",
    "        \n",
    "    def select_arm(self) -> int:\n",
    "        return random.randint(0, self.n_arms - 1)\n",
    "        \n",
    "    def update(self, arm: int, reward: float):\n",
    "        self.arm_counts[arm] += 1\n",
    "        self.arm_rewards[arm] += reward\n",
    "\n",
    "# Run random policy\n",
    "env = ClinicalTrialEnvironment('Clinical_Trial.csv')\n",
    "policy = RandomPolicy(env.n_arms)\n",
    "rewards = []\n",
    "arm_selections = []\n",
    "\n",
    "for i in range(N_ITERATIONS):\n",
    "    arm = policy.select_arm()\n",
    "    reward = env.get_reward(arm)\n",
    "    policy.update(arm, reward)\n",
    "    rewards.append(reward)\n",
    "    arm_selections.append(arm)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Iteration {i+1}: Selected arm {arm}, Reward: {reward}\")\n",
    "\n",
    "print(f\"\\nTotal reward: {sum(rewards)}\")\n",
    "print(f\"Arm selection counts: {np.bincount(arm_selections, minlength=env.n_arms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Greedy Policy (1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GreedyPolicy:\n",
    "    def __init__(self, n_arms: int):\n",
    "        self.n_arms = n_arms\n",
    "        self.arm_counts = np.zeros(n_arms)\n",
    "        self.arm_rewards = np.zeros(n_arms)\n",
    "        \n",
    "    def select_arm(self) -> int:\n",
    "        if np.sum(self.arm_counts) == 0:\n",
    "            return random.randint(0, self.n_arms - 1)\n",
    "        return np.argmax(self.arm_rewards / (self.arm_counts + 1e-10))\n",
    "        \n",
    "    def update(self, arm: int, reward: float):\n",
    "        self.arm_counts[arm] += 1\n",
    "        self.arm_rewards[arm] += reward\n",
    "\n",
    "# Run greedy policy\n",
    "env = ClinicalTrialEnvironment('Clinical_Trial.csv')\n",
    "policy = GreedyPolicy(env.n_arms)\n",
    "rewards = []\n",
    "arm_selections = []\n",
    "\n",
    "for i in range(N_ITERATIONS):\n",
    "    arm = policy.select_arm()\n",
    "    reward = env.get_reward(arm)\n",
    "    policy.update(arm, reward)\n",
    "    rewards.append(reward)\n",
    "    arm_selections.append(arm)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Iteration {i+1}: Selected arm {arm}, Reward: {reward}\")\n",
    "\n",
    "print(f\"\\nTotal reward: {sum(rewards)}\")\n",
    "print(f\"Arm selection counts: {np.bincount(arm_selections, minlength=env.n_arms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Epsilon-Greedy Policy (1.5M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, n_arms: int, epsilon: float):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.arm_counts = np.zeros(n_arms)\n",
    "        self.arm_rewards = np.zeros(n_arms)\n",
    "        \n",
    "    def select_arm(self) -> int:\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_arms - 1)\n",
    "        if np.sum(self.arm_counts) == 0:\n",
    "            return random.randint(0, self.n_arms - 1)\n",
    "        return np.argmax(self.arm_rewards / (self.arm_counts + 1e-10))\n",
    "        \n",
    "    def update(self, arm: int, reward: float):\n",
    "        self.arm_counts[arm] += 1\n",
    "        self.arm_rewards[arm] += reward\n",
    "\n",
    "# Run epsilon-greedy policies with different epsilon values\n",
    "env = ClinicalTrialEnvironment('Clinical_Trial.csv')\n",
    "epsilon_results = {}\n",
    "\n",
    "for epsilon in EPSILON_VALUES:\n",
    "    print(f\"\\nRunning Epsilon-Greedy with Îµ = {epsilon}\")\n",
    "    policy = EpsilonGreedyPolicy(env.n_arms, epsilon)\n",
    "    rewards = []\n",
    "    arm_selections = []\n",
    "    \n",
    "    for i in range(N_ITERATIONS):\n",
    "        arm = policy.select_arm()\n",
    "        reward = env.get_reward(arm)\n",
    "        policy.update(arm, reward)\n",
    "        rewards.append(reward)\n",
    "        arm_selections.append(arm)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Iteration {i+1}: Selected arm {arm}, Reward: {reward}\")\n",
    "    \n",
    "    epsilon_results[epsilon] = rewards\n",
    "    print(f\"Total reward: {sum(rewards)}\")\n",
    "    print(f\"Arm selection counts: {np.bincount(arm_selections, minlength=env.n_arms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using UCB (1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class UCBPolicy:\n",
    "    def __init__(self, n_arms: int, c: float = 2.0):\n",
    "        self.n_arms = n_arms\n",
    "        self.c = c\n",
    "        self.arm_counts = np.zeros(n_arms)\n",
    "        self.arm_rewards = np.zeros(n_arms)\n",
    "        \n",
    "    def select_arm(self) -> int:\n",
    "        if np.sum(self.arm_counts) == 0:\n",
    "            return random.randint(0, self.n_arms - 1)\n",
    "        \n",
    "        t = np.sum(self.arm_counts)\n",
    "        ucb_values = (self.arm_rewards / (self.arm_counts + 1e-10)) + \\\n",
    "                     self.c * np.sqrt(np.log(t) / (self.arm_counts + 1e-10))\n",
    "        return np.argmax(ucb_values)\n",
    "        \n",
    "    def update(self, arm: int, reward: float):\n",
    "        self.arm_counts[arm] += 1\n",
    "        self.arm_rewards[arm] += reward\n",
    "\n",
    "# Run UCB policy\n",
    "env = ClinicalTrialEnvironment('Clinical_Trial.csv')\n",
    "policy = UCBPolicy(env.n_arms, UCB_C)\n",
    "rewards = []\n",
    "arm_selections = []\n",
    "\n",
    "for i in range(N_ITERATIONS):\n",
    "    arm = policy.select_arm()\n",
    "    reward = env.get_reward(arm)\n",
    "    policy.update(arm, reward)\n",
    "    rewards.append(reward)\n",
    "    arm_selections.append(arm)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Iteration {i+1}: Selected arm {arm}, Reward: {reward}\")\n",
    "\n",
    "print(f\"\\nTotal reward: {sum(rewards)}\")\n",
    "print(f\"Arm selection counts: {np.bincount(arm_selections, minlength=env.n_arms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the cumulative rewards for all policies (0.5M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run all policies and collect results\n",
    "env = ClinicalTrialEnvironment('Clinical_Trial.csv')\n",
    "policies = {\n",
    "    'Random': RandomPolicy(env.n_arms),\n",
    "    'Greedy': GreedyPolicy(env.n_arms),\n",
    "    'UCB': UCBPolicy(env.n_arms, UCB_C)\n",
    "}\n",
    "\n",
    "# Add epsilon-greedy policies\n",
    "for epsilon in EPSILON_VALUES:\n",
    "    policies[f'Epsilon-Greedy (Îµ={epsilon})'] = EpsilonGreedyPolicy(env.n_arms, epsilon)\n",
    "\n",
    "# Run simulations and collect results\n",
    "results = {}\n",
    "for name, policy in policies.items():\n",
    "    rewards = []\n",
    "    for _ in range(N_ITERATIONS):\n",
    "        arm = policy.select_arm()\n",
    "        reward = env.get_reward(arm)\n",
    "        policy.update(arm, reward)\n",
    "        rewards.append(reward)\n",
    "    results[name] = rewards\n",
    "\n",
    "# Plot cumulative rewards\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, rewards in results.items():\n",
    "    cumulative_rewards = np.cumsum(rewards)\n",
    "    plt.plot(cumulative_rewards, label=name)\n",
    "\n",
    "plt.title('Cumulative Rewards Comparison')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot arm selection frequencies\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, policy in policies.items():\n",
    "    arm_counts = np.bincount(arm_selections, minlength=env.n_arms)\n",
    "    plt.bar(np.arange(env.n_arms) + 0.1 * list(policies.keys()).index(name),\n",
    "            arm_counts / N_ITERATIONS,\n",
    "            width=0.1,\n",
    "            label=name)\n",
    "\n",
    "plt.title('Arm Selection Frequencies')\n",
    "plt.xlabel('Arm')\n",
    "plt.ylabel('Selection Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion (0.5M)\n",
    "\n",
    "Based on the simulation results, we can draw several important conclusions about the effectiveness of different Multi-Armed Bandit (MAB) policies in the clinical trial scenario:\n",
    "\n",
    "1. **Policy Performance Comparison**:\n",
    "   - The UCB policy demonstrated the best overall performance, achieving higher cumulative rewards than other policies. This is because UCB effectively balances exploration and exploitation by considering both the average reward and the uncertainty in our estimates.\n",
    "   - Among the Îµ-Greedy policies, Îµ=0.1 performed better than Îµ=0.2 and Îµ=0.5, suggesting that a smaller exploration rate is more effective in this scenario.\n",
    "   - The Greedy policy performed better than Random but worse than UCB and Îµ-Greedy, highlighting the importance of exploration in finding the optimal treatment.\n",
    "\n",
    "2. **Exploration vs Exploitation**:\n",
    "   - The Random policy, while having the lowest performance, provided valuable insights into the baseline performance and the importance of structured exploration.\n",
    "   - The Îµ-Greedy policy's performance variation with different Îµ values demonstrated the critical balance between exploration and exploitation. A smaller Îµ value (0.1) allowed for more exploitation of the best-known treatment while maintaining sufficient exploration.\n",
    "   - The UCB policy's superior performance can be attributed to its adaptive exploration strategy, which automatically adjusts the exploration rate based on the uncertainty in our estimates.\n",
    "\n",
    "3. **Clinical Implications**:\n",
    "   - The results suggest that adaptive treatment selection using MAB algorithms can significantly improve patient outcomes in clinical trials.\n",
    "   - The UCB policy's success indicates that considering both the expected reward and the uncertainty in treatment effectiveness is crucial for optimal treatment selection.\n",
    "   - The findings support the use of adaptive clinical trial designs that can efficiently identify the most effective treatment while minimizing the number of patients exposed to suboptimal treatments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
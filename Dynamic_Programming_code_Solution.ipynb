{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e826906",
   "metadata": {},
   "source": [
    "### The Smart Supplier: Optimizing Orders in a Fluctuating Market - 6 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beffcce",
   "metadata": {},
   "source": [
    "Develop a reinforcement learning agent using dynamic programming to help a Smart Supplier decide which products to manufacture and sell each day to maximize profit. The agent must learn the optimal policy for choosing daily production quantities, considering its limited raw materials and the unpredictable daily demand and selling prices for different products.\n",
    "\n",
    "#### **Scenario**\n",
    " A small Smart Supplier manufactures two simple products: Product A and Product B. Each day, the supplier has a limited amount of raw material. The challenge is that the market demand and selling price for Product A and Product B change randomly each day, making some products more profitable than others at different times. The supplier needs to decide how much of each product to produce to maximize profit while managing their limited raw material.\n",
    "\n",
    "#### **Objective**\n",
    "The Smart Supplier's agent must learn the optimal policy π∗ using dynamic programming (Value Iteration or Policy Iteration) to decide how many units of Product A and Product B to produce each day to maximize the total profit over the fixed number of days, given the daily changing market conditions and limited raw material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4feac",
   "metadata": {},
   "source": [
    "### --- 1. Custom Environment Creation (SmartSupplierEnv) --- ( 1 Mark )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a4a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class SmartSupplierEnv:\n",
    "    def __init__(self, num_days=5, initial_raw_material=10):\n",
    "        # Environment parameters\n",
    "        self.num_days = num_days\n",
    "        self.initial_raw_material = initial_raw_material\n",
    "        \n",
    "        # Market states and their product prices\n",
    "        self.market_states = {\n",
    "            1: {'A_price': 8, 'B_price': 2},  # High Demand for A\n",
    "            2: {'A_price': 3, 'B_price': 5}   # High Demand for B\n",
    "        }\n",
    "        \n",
    "        # Product raw material costs\n",
    "        self.rm_costs = {'A': 2, 'B': 1}\n",
    "        \n",
    "        # Action space definition\n",
    "        self.actions = {\n",
    "            0: {'A': 2, 'B': 0},  # Produce_2A_0B\n",
    "            1: {'A': 1, 'B': 2},  # Produce_1A_2B\n",
    "            2: {'A': 0, 'B': 5},  # Produce_0A_5B\n",
    "            3: {'A': 3, 'B': 0},  # Produce_3A_0B\n",
    "            4: {'A': 0, 'B': 0}   # Do_Nothing\n",
    "        }\n",
    "        \n",
    "        # State space dimensions\n",
    "        self.state_dimensions = {\n",
    "            'day': range(1, num_days + 1),\n",
    "            'raw_material': range(initial_raw_material + 1),\n",
    "            'market_state': [1, 2]\n",
    "        }\n",
    "        \n",
    "        # Initialize state\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to initial state\"\"\"\n",
    "        self.current_day = 1\n",
    "        self.current_raw_material = self.initial_raw_material\n",
    "        self.current_market_state = random.choice([1, 2])\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Return current state as a tuple\"\"\"\n",
    "        return (self.current_day, self.current_raw_material, self.current_market_state)\n",
    "    \n",
    "    def calculate_reward(self, action):\n",
    "        \"\"\"Calculate reward for taking an action in current state\"\"\"\n",
    "        action_quantities = self.actions[action]\n",
    "        \n",
    "        # Calculate required raw material\n",
    "        required_rm = (action_quantities['A'] * self.rm_costs['A'] + \n",
    "                      action_quantities['B'] * self.rm_costs['B'])\n",
    "        \n",
    "        # Check if enough raw material\n",
    "        if required_rm > self.current_raw_material:\n",
    "            return 0  # Action fails, no reward\n",
    "        \n",
    "        # Calculate profit\n",
    "        profit = (action_quantities['A'] * self.market_states[self.current_market_state]['A_price'] +\n",
    "                 action_quantities['B'] * self.market_states[self.current_market_state]['B_price'])\n",
    "        \n",
    "        return profit\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment\"\"\"\n",
    "        # Calculate reward\n",
    "        reward = self.calculate_reward(action)\n",
    "        \n",
    "        # Update state\n",
    "        action_quantities = self.actions[action]\n",
    "        required_rm = (action_quantities['A'] * self.rm_costs['A'] + \n",
    "                      action_quantities['B'] * self.rm_costs['B'])\n",
    "        \n",
    "        if required_rm <= self.current_raw_material:\n",
    "            self.current_raw_material -= required_rm\n",
    "        \n",
    "        # Move to next day\n",
    "        self.current_day += 1\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.current_day > self.num_days\n",
    "        \n",
    "        if not done:\n",
    "            # Reset raw material for next day\n",
    "            self.current_raw_material = self.initial_raw_material\n",
    "            # Randomly change market state\n",
    "            self.current_market_state = random.choice([1, 2])\n",
    "        \n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b2487",
   "metadata": {},
   "source": [
    "### --- 2. Dynamic Programming Implementation (Value Iteration) --- (2 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027db857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=1.0, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"Value Iteration algorithm to find optimal policy\"\"\"\n",
    "    # Initialize value function\n",
    "    V = {}\n",
    "    for day in env.state_dimensions['day']:\n",
    "        for rm in env.state_dimensions['raw_material']:\n",
    "            for market in env.state_dimensions['market_state']:\n",
    "                V[(day, rm, market)] = 0\n",
    "    \n",
    "    # Initialize terminal state values\n",
    "    for rm in env.state_dimensions['raw_material']:\n",
    "        for market in env.state_dimensions['market_state']:\n",
    "            V[(env.num_days + 1, rm, market)] = 0\n",
    "    \n",
    "    # Value iteration\n",
    "    for i in range(max_iterations):\n",
    "        delta = 0\n",
    "        # Update each state\n",
    "        for day in env.state_dimensions['day']:\n",
    "            for rm in env.state_dimensions['raw_material']:\n",
    "                for market in env.state_dimensions['market_state']:\n",
    "                    # Set current state\n",
    "                    env.current_day = day\n",
    "                    env.current_raw_material = rm\n",
    "                    env.current_market_state = market\n",
    "                    \n",
    "                    # Find best action value\n",
    "                    best_value = float('-inf')\n",
    "                    for action in range(len(env.actions)):\n",
    "                        next_state, reward, done = env.step(action)\n",
    "                        value = reward + gamma * V[next_state]\n",
    "                        best_value = max(best_value, value)\n",
    "                        # Reset state for next action evaluation\n",
    "                        env.current_day = day\n",
    "                        env.current_raw_material = rm\n",
    "                        env.current_market_state = market\n",
    "                    \n",
    "                    # Update value function\n",
    "                    delta = max(delta, abs(V[(day, rm, market)] - best_value))\n",
    "                    V[(day, rm, market)] = best_value\n",
    "        \n",
    "        # Check convergence\n",
    "        if delta < theta:\n",
    "            print(f\"Value iteration converged after {i+1} iterations\")\n",
    "            break\n",
    "    \n",
    "    # Extract optimal policy\n",
    "    policy = {}\n",
    "    for day in env.state_dimensions['day']:\n",
    "        for rm in env.state_dimensions['raw_material']:\n",
    "            for market in env.state_dimensions['market_state']:\n",
    "                best_action = None\n",
    "                best_value = float('-inf')\n",
    "                \n",
    "                env.current_day = day\n",
    "                env.current_raw_material = rm\n",
    "                env.current_market_state = market\n",
    "                \n",
    "                for action in range(len(env.actions)):\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    value = reward + gamma * V[next_state]\n",
    "                    if value > best_value:\n",
    "                        best_value = value\n",
    "                        best_action = action\n",
    "                    env.current_day = day\n",
    "                    env.current_raw_material = rm\n",
    "                    env.current_market_state = market\n",
    "                \n",
    "                policy[(day, rm, market)] = best_action\n",
    "    \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cfa71",
   "metadata": {},
   "source": [
    "#### --- 3. Simulation and Policy Analysis ---  ( 1 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62490896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(env, policy, num_episodes=1000):\n",
    "    \"\"\"Simulate the learned policy over multiple episodes\"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    return np.mean(total_rewards), np.std(total_rewards)\n",
    "\n",
    "def analyze_policy(policy, env):\n",
    "    \"\"\"Analyze and print snippets of the learned optimal policy\"\"\"\n",
    "    print(\"\\nPolicy Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Analyze policy for different market states\n",
    "    for market in [1, 2]:\n",
    "        print(f\"\\nMarket State {market}:\")\n",
    "        print(f\"Market State {market} prices: A=${env.market_states[market]['A_price']}, B=${env.market_states[market]['B_price']}\")\n",
    "        \n",
    "        for day in range(1, env.num_days + 1):\n",
    "            print(f\"\\nDay {day}:\")\n",
    "            for rm in range(env.initial_raw_material + 1):\n",
    "                action = policy[(day, rm, market)]\n",
    "                action_desc = {\n",
    "                    0: \"Produce_2A_0B\",\n",
    "                    1: \"Produce_1A_2B\",\n",
    "                    2: \"Produce_0A_5B\",\n",
    "                    3: \"Produce_3A_0B\",\n",
    "                    4: \"Do_Nothing\"\n",
    "                }[action]\n",
    "                print(f\"RM={rm}: {action_desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305647e",
   "metadata": {},
   "source": [
    "#### --- 4. Impact of Dynamics Analysis --- (1 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2448380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy Analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "Market State 1:\n",
      "Market State 1 prices: A=$8, B=$2\n",
      "\n",
      "Day 1:\n",
      "RM=0: Produce_2A_0B\n",
      "RM=1: Do_Nothing\n",
      "RM=2: Produce_2A_0B\n",
      "RM=3: Do_Nothing\n",
      "RM=4: Produce_2A_0B\n",
      "RM=5: Produce_2A_0B\n",
      "RM=6: Produce_3A_0B\n",
      "RM=7: Produce_3A_0B\n",
      "RM=8: Produce_3A_0B\n",
      "RM=9: Produce_3A_0B\n",
      "RM=10: Produce_3A_0B\n",
      "\n",
      "Day 2:\n",
      "RM=0: Produce_2A_0B\n",
      "RM=1: Do_Nothing\n",
      "RM=2: Produce_3A_0B\n",
      "RM=3: Produce_3A_0B\n",
      "RM=4: Produce_2A_0B\n",
      "RM=5: Produce_2A_0B\n",
      "RM=6: Produce_3A_0B\n",
      "RM=7: Produce_3A_0B\n",
      "RM=8: Produce_3A_0B\n",
      "RM=9: Produce_3A_0B\n",
      "RM=10: Produce_3A_0B\n",
      "\n",
      "Day 3:\n",
      "RM=0: Produce_0A_5B\n",
      "RM=1: Do_Nothing\n",
      "RM=2: Produce_1A_2B\n",
      "RM=3: Produce_2A_0B\n",
      "RM=4: Produce_2A_0B\n",
      "RM=5: Produce_2A_0B\n",
      "RM=6: Produce_3A_0B\n",
      "RM=7: Produce_3A_0B\n",
      "RM=8: Produce_3A_0B\n",
      "RM=9: Produce_3A_0B\n",
      "RM=10: Produce_3A_0B\n",
      "\n",
      "Day 4:\n",
      "RM=0: Produce_1A_2B\n",
      "RM=1: Produce_2A_0B\n",
      "RM=2: Produce_2A_0B\n",
      "RM=3: Produce_2A_0B\n",
      "RM=4: Produce_2A_0B\n",
      "RM=5: Produce_2A_0B\n",
      "RM=6: Produce_3A_0B\n",
      "RM=7: Produce_3A_0B\n",
      "RM=8: Produce_3A_0B\n",
      "RM=9: Produce_3A_0B\n",
      "RM=10: Produce_3A_0B\n",
      "\n",
      "Day 5:\n",
      "RM=0: Produce_2A_0B\n",
      "RM=1: Produce_2A_0B\n",
      "RM=2: Produce_2A_0B\n",
      "RM=3: Produce_2A_0B\n",
      "RM=4: Produce_2A_0B\n",
      "RM=5: Produce_2A_0B\n",
      "RM=6: Produce_3A_0B\n",
      "RM=7: Produce_3A_0B\n",
      "RM=8: Produce_3A_0B\n",
      "RM=9: Produce_3A_0B\n",
      "RM=10: Produce_3A_0B\n",
      "\n",
      "Market State 2:\n",
      "Market State 2 prices: A=$3, B=$5\n",
      "\n",
      "Day 1:\n",
      "RM=0: Produce_1A_2B\n",
      "RM=1: Produce_0A_5B\n",
      "RM=2: Produce_2A_0B\n",
      "RM=3: Produce_1A_2B\n",
      "RM=4: Produce_1A_2B\n",
      "RM=5: Produce_0A_5B\n",
      "RM=6: Produce_0A_5B\n",
      "RM=7: Produce_0A_5B\n",
      "RM=8: Produce_0A_5B\n",
      "RM=9: Produce_0A_5B\n",
      "RM=10: Produce_0A_5B\n",
      "\n",
      "Day 2:\n",
      "RM=0: Produce_2A_0B\n",
      "RM=1: Produce_2A_0B\n",
      "RM=2: Produce_2A_0B\n",
      "RM=3: Produce_1A_2B\n",
      "RM=4: Produce_1A_2B\n",
      "RM=5: Produce_0A_5B\n",
      "RM=6: Produce_0A_5B\n",
      "RM=7: Produce_0A_5B\n",
      "RM=8: Produce_0A_5B\n",
      "RM=9: Produce_0A_5B\n",
      "RM=10: Produce_0A_5B\n",
      "\n",
      "Day 3:\n",
      "RM=0: Do_Nothing\n",
      "RM=1: Produce_2A_0B\n",
      "RM=2: Produce_1A_2B\n",
      "RM=3: Produce_2A_0B\n",
      "RM=4: Produce_1A_2B\n",
      "RM=5: Produce_0A_5B\n",
      "RM=6: Produce_0A_5B\n",
      "RM=7: Produce_0A_5B\n",
      "RM=8: Produce_0A_5B\n",
      "RM=9: Produce_0A_5B\n",
      "RM=10: Produce_0A_5B\n",
      "\n",
      "Day 4:\n",
      "RM=0: Do_Nothing\n",
      "RM=1: Produce_1A_2B\n",
      "RM=2: Produce_2A_0B\n",
      "RM=3: Produce_1A_2B\n",
      "RM=4: Produce_1A_2B\n",
      "RM=5: Produce_0A_5B\n",
      "RM=6: Produce_0A_5B\n",
      "RM=7: Produce_0A_5B\n",
      "RM=8: Produce_0A_5B\n",
      "RM=9: Produce_0A_5B\n",
      "RM=10: Produce_0A_5B\n",
      "\n",
      "Day 5:\n",
      "RM=0: Produce_2A_0B\n",
      "RM=1: Produce_2A_0B\n",
      "RM=2: Produce_2A_0B\n",
      "RM=3: Produce_2A_0B\n",
      "RM=4: Produce_1A_2B\n",
      "RM=5: Produce_0A_5B\n",
      "RM=6: Produce_0A_5B\n",
      "RM=7: Produce_0A_5B\n",
      "RM=8: Produce_0A_5B\n",
      "RM=9: Produce_0A_5B\n",
      "RM=10: Produce_0A_5B\n",
      "\n",
      "Average reward over 1000 episodes: 122.41 ± 1.11\n",
      "\n",
      "Value function for key states:\n",
      "Day 1, RM=10, Market=1: 123.00\n",
      "Day 1, RM=10, Market=2: 123.00\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Create environment\n",
    "    env = SmartSupplierEnv()\n",
    "    \n",
    "    # Run value iteration\n",
    "    V, policy = value_iteration(env)\n",
    "    \n",
    "    # Analyze policy\n",
    "    analyze_policy(policy, env)\n",
    "    \n",
    "    # Simulate policy\n",
    "    mean_reward, std_reward = simulate_policy(env, policy)\n",
    "    print(f\"\\nAverage reward over 1000 episodes: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "    \n",
    "    # Print value function for key states\n",
    "    print(\"\\nValue function for key states:\")\n",
    "    print(f\"Day 1, RM=10, Market=1: {V[(1, 10, 1)]:.2f}\")\n",
    "    print(f\"Day 1, RM=10, Market=2: {V[(1, 10, 2)]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
